# Synthetic Log Stream Project

This project implements a pipeline that generates and processes synthetic log streams. The pipeline is designed to simulate log data generation and subsequent processing, which can be useful for testing log analysis tools and procedures.

## Log Generator Node

The `LogGeneratorNode` is responsible for creating synthetic log entries. Each log entry includes a timestamp, a log level, and a message with a secure random value. The node uses the `secrets` library to ensure the use of a secure random generator for any cryptographic purposes. It produces these entries to the `synthetic_log_stream` dataset.

## Log Processor Node

The `LogProcessorNode` consumes the synthetic log entries generated by the `LogGeneratorNode`. It processes each entry, which could include parsing, filtering, or transforming the data. After processing, the entries are produced to the `processed_log_stream` dataset.

## Pipeline Configuration

The pipeline is configured in `pipeline.yml`, which defines the nodes and datasets. The `LogGenerator` node outputs to the `synthetic_log_stream` dataset, and the `LogProcessor` node inputs from the `synthetic_log_stream` and outputs to the `processed_log_stream` dataset.

The datasets are configured as Kafka streams, allowing for real-time data processing and streaming capabilities.

## Usage

To run the pipeline, ensure that the Aineko framework is installed and configured correctly. Then, execute the pipeline using the provided `pipeline.yml` configuration file. The pipeline will start generating and processing synthetic log streams in real-time.
